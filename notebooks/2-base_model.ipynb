{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Defining the base models\n",
    "\n",
    "<h4>In this chapter we will build some base models including a naive/simple model and we use some defined functions to get our data to do so. </h4>\n",
    "The problem is a classification problem so that is the key driver of our decisions\n",
    "\n",
    "The following models are build:\n",
    "\n",
    "<li> A naive/simple model </li>\n",
    "<li> A randomforest model </li>\n",
    "<li> A initial simple neural network </li>\n",
    "<li> A deeper neural </li>\n",
    "\n",
    "<p>\n",
    "The simple model functions as a simple baseline. We should be able to defeat this model otherwise the project would not make sense at all.\n",
    "We also train a random forest to see how a traditional model performs with the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huube\\OneDrive\\Master of Informatics\\Machine Learning\\Eindopdracht\\.venv\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "2022-02-13 13:39:14.914 | INFO     | src.data.make_dataset:create_train_test_validation:73 - found file labeled_data.csv, procceed with creating train, test and validation sets\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from loguru import logger\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from definitions import get_project_root\n",
    "from src.data.make_dataset import create_train_test_validation\n",
    "from src.visualization.visualize import plot_results\n",
    "from src.models.train_model import simple_baseline\n",
    "\n",
    "root = get_project_root()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 13:39:21.608 | INFO     | src.data.make_dataset:create_train_test_validation:73 - found file labeled_data.csv, procceed with creating train, test and validation sets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((61711, 23), (61711, 1), (13225, 23), (13225, 1), (13224, 23), (13224, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create train, validation and test sets\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = create_train_test_validation()\n",
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize empty scores\n",
    "result = {}\n",
    "score = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3842256503327284"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load in the simple model (prediction: song = rock)\n",
    "\n",
    "score['simple_baseline'] = simple_baseline()\n",
    "score['simple_baseline']\n",
    "\n",
    "## So if we always predict the genre being 'Rock', we'd have a accuracy of 38.4%. That's due to a signifiacnt class imbalance as we already observed during the EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we always predict the genre being 'Rock', we'd have a accuracy of 38.4%. That's due to a signifiacnt class imbalance as we already observed during the EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huube\\AppData\\Local\\Temp\\ipykernel_15160\\551774697.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_clf.fit(x_train,y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Simple decision tree\n",
    "rf_clf = RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "rf_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5569419237749547"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score['randomforest'] = rf_clf.score(x_test,y_test)\n",
    "score['randomforest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the randomforest hits an accuracy of 55.7%.\n",
    "Since the objective of this course is to deliver neural networks, we won't dive any deeper into trying to optimize it.\n",
    "\n",
    "Can our first base model beat the random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the base model, we made the following assumptions:\n",
    "\n",
    "<li> We use the sequential API.\n",
    "<li> We use the activation function 'relu'\n",
    "<li> We Need to use a softmax activation on the output layer\n",
    "<li> We use the sparse_categorical_crossentropy as loss function due to our categorical\n",
    "<li> We use early stopping to stop the model when it is not learning anymore\n",
    "<li> We use three layers. 1 input, 1 hidden and 1 output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Dense)                (None, 23)                552       \n",
      "_________________________________________________________________\n",
      "hidden_1 (Dense)             (None, 100)               2400      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 15)                1515      \n",
      "=================================================================\n",
      "Total params: 4,467\n",
      "Trainable params: 4,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1929/1929 [==============================] - 3s 2ms/step - loss: 1.5352 - accuracy: 0.5068 - val_loss: 1.4401 - val_accuracy: 0.5298\n",
      "Epoch 2/100\n",
      "1929/1929 [==============================] - 3s 1ms/step - loss: 1.4165 - accuracy: 0.5357 - val_loss: 1.4035 - val_accuracy: 0.5398\n",
      "Epoch 3/100\n",
      "1929/1929 [==============================] - 3s 2ms/step - loss: 1.3899 - accuracy: 0.5419 - val_loss: 1.4005 - val_accuracy: 0.5442\n",
      "Epoch 4/100\n",
      "1929/1929 [==============================] - 3s 2ms/step - loss: 1.3738 - accuracy: 0.5472 - val_loss: 1.3797 - val_accuracy: 0.5469\n",
      "Epoch 5/100\n",
      "1929/1929 [==============================] - 3s 2ms/step - loss: 1.3612 - accuracy: 0.5510 - val_loss: 1.3688 - val_accuracy: 0.5501\n",
      "Epoch 6/100\n",
      "1929/1929 [==============================] - 3s 2ms/step - loss: 1.3516 - accuracy: 0.5545 - val_loss: 1.3667 - val_accuracy: 0.5498\n",
      "Epoch 7/100\n",
      "1929/1929 [==============================] - 3s 2ms/step - loss: 1.3441 - accuracy: 0.5559 - val_loss: 1.3692 - val_accuracy: 0.5484\n",
      "Epoch 8/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.3380 - accuracy: 0.5582 - val_loss: 1.3592 - val_accuracy: 0.5505\n",
      "Epoch 9/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.3326 - accuracy: 0.5600 - val_loss: 1.3578 - val_accuracy: 0.5524\n",
      "Epoch 10/100\n",
      "1929/1929 [==============================] - 3s 1ms/step - loss: 1.3280 - accuracy: 0.5609 - val_loss: 1.3534 - val_accuracy: 0.5551\n",
      "Epoch 11/100\n",
      "1929/1929 [==============================] - 4s 2ms/step - loss: 1.3233 - accuracy: 0.5622 - val_loss: 1.3472 - val_accuracy: 0.5576\n",
      "Epoch 12/100\n",
      "1929/1929 [==============================] - 3s 2ms/step - loss: 1.3194 - accuracy: 0.5638 - val_loss: 1.3482 - val_accuracy: 0.5586\n",
      "Epoch 13/100\n",
      "1929/1929 [==============================] - 3s 2ms/step - loss: 1.3167 - accuracy: 0.5633 - val_loss: 1.3484 - val_accuracy: 0.5574\n",
      "Epoch 14/100\n",
      "1929/1929 [==============================] - 3s 2ms/step - loss: 1.3137 - accuracy: 0.5655 - val_loss: 1.3502 - val_accuracy: 0.5547\n",
      "Epoch 15/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.3113 - accuracy: 0.5659 - val_loss: 1.3474 - val_accuracy: 0.5572\n",
      "Epoch 16/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.3092 - accuracy: 0.5665 - val_loss: 1.3464 - val_accuracy: 0.5595\n",
      "Epoch 17/100\n",
      "1929/1929 [==============================] - 2s 960us/step - loss: 1.3063 - accuracy: 0.5685 - val_loss: 1.3502 - val_accuracy: 0.5551\n",
      "Epoch 18/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.3053 - accuracy: 0.5672 - val_loss: 1.3461 - val_accuracy: 0.5568\n",
      "Epoch 19/100\n",
      "1929/1929 [==============================] - 2s 947us/step - loss: 1.3023 - accuracy: 0.5693 - val_loss: 1.3511 - val_accuracy: 0.5598\n",
      "Epoch 20/100\n",
      "1929/1929 [==============================] - 2s 992us/step - loss: 1.3009 - accuracy: 0.5692 - val_loss: 1.3423 - val_accuracy: 0.5602\n",
      "Epoch 21/100\n",
      "1929/1929 [==============================] - 2s 966us/step - loss: 1.2997 - accuracy: 0.5703 - val_loss: 1.3455 - val_accuracy: 0.5577\n",
      "Epoch 22/100\n",
      "1929/1929 [==============================] - 2s 983us/step - loss: 1.2972 - accuracy: 0.5710 - val_loss: 1.3451 - val_accuracy: 0.5599\n",
      "Epoch 23/100\n",
      "1929/1929 [==============================] - 2s 988us/step - loss: 1.2960 - accuracy: 0.5707 - val_loss: 1.3433 - val_accuracy: 0.5596\n",
      "Epoch 24/100\n",
      "1929/1929 [==============================] - 2s 959us/step - loss: 1.2956 - accuracy: 0.5710 - val_loss: 1.3439 - val_accuracy: 0.5605\n",
      "Epoch 25/100\n",
      "1929/1929 [==============================] - 2s 980us/step - loss: 1.2938 - accuracy: 0.5719 - val_loss: 1.3501 - val_accuracy: 0.5569\n",
      "414/414 [==============================] - 0s 637us/step - loss: 1.3467 - accuracy: 0.5503\n"
     ]
    }
   ],
   "source": [
    "## first neural network.\n",
    "\n",
    "early_stop = EarlyStopping(patience=5,restore_best_weights=True)\n",
    "tensorboard_callback = TensorBoard(log_dir = root / 'src' / 'logs',histogram_freq=1) \n",
    "\n",
    "base_model = Sequential(\n",
    "    [   \n",
    "        Dense(23,activation='relu', name = 'input',input_shape=(len(x_train[0]),)),\n",
    "        Dense(100,activation='relu', name = 'hidden_1'),\n",
    "        Dense(15, activation='softmax',name='output')\n",
    "    ]\n",
    ")\n",
    "\n",
    "base_model.summary()\n",
    "base_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy']) \n",
    "result['base'] = base_model.fit(x_train, y_train, epochs = 100, validation_data=(x_valid,y_valid),callbacks=[early_stop,tensorboard_callback],verbose=1)\n",
    "score['base'] = base_model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3466856479644775, 0.5502873659133911]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score['base']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training we see that the validation accuracy is moving towares 56% which would beat the random forest by a small margin but unfortunetaly on the test set, the base model scores an accuracy of 55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## let's have a look at the results\n",
    "##plot_results(result,ymin=0,ymax=1,yscale=\"linear\")\n",
    "%tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Adding more complexity to baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if we can improve this, we are going to try and deepen the model a bit to see if more layers add beter results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Dense)                (None, 23)                552       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 150)               3600      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                7550      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 15)                765       \n",
      "=================================================================\n",
      "Total params: 17,567\n",
      "Trainable params: 17,567\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1929/1929 [==============================] - 4s 2ms/step - loss: 1.5467 - accuracy: 0.5051 - val_loss: 1.4625 - val_accuracy: 0.5199\n",
      "Epoch 2/100\n",
      "1929/1929 [==============================] - 4s 2ms/step - loss: 1.4273 - accuracy: 0.5327 - val_loss: 1.4237 - val_accuracy: 0.5340\n",
      "Epoch 3/100\n",
      "1929/1929 [==============================] - 4s 2ms/step - loss: 1.3970 - accuracy: 0.5416 - val_loss: 1.4074 - val_accuracy: 0.5412\n",
      "Epoch 4/100\n",
      "1929/1929 [==============================] - 4s 2ms/step - loss: 1.3775 - accuracy: 0.5459 - val_loss: 1.3890 - val_accuracy: 0.5452\n",
      "Epoch 5/100\n",
      "1929/1929 [==============================] - 3s 2ms/step - loss: 1.3625 - accuracy: 0.5494 - val_loss: 1.3840 - val_accuracy: 0.5440\n",
      "Epoch 6/100\n",
      "1929/1929 [==============================] - 3s 1ms/step - loss: 1.3501 - accuracy: 0.5542 - val_loss: 1.3879 - val_accuracy: 0.5501\n",
      "Epoch 7/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.3403 - accuracy: 0.5576 - val_loss: 1.3974 - val_accuracy: 0.5371\n",
      "Epoch 8/100\n",
      "1929/1929 [==============================] - 3s 1ms/step - loss: 1.3309 - accuracy: 0.5605 - val_loss: 1.3749 - val_accuracy: 0.5490\n",
      "Epoch 9/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.3205 - accuracy: 0.5636 - val_loss: 1.3699 - val_accuracy: 0.5518\n",
      "Epoch 10/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.3152 - accuracy: 0.5651 - val_loss: 1.3706 - val_accuracy: 0.5530\n",
      "Epoch 11/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.3094 - accuracy: 0.5666 - val_loss: 1.3707 - val_accuracy: 0.5490\n",
      "Epoch 12/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.3018 - accuracy: 0.5688 - val_loss: 1.3647 - val_accuracy: 0.5576\n",
      "Epoch 13/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.2971 - accuracy: 0.5700 - val_loss: 1.3747 - val_accuracy: 0.5429\n",
      "Epoch 14/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.2905 - accuracy: 0.5710 - val_loss: 1.3833 - val_accuracy: 0.5439\n",
      "Epoch 15/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.2868 - accuracy: 0.5733 - val_loss: 1.3660 - val_accuracy: 0.5571\n",
      "Epoch 16/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.2811 - accuracy: 0.5743 - val_loss: 1.3671 - val_accuracy: 0.5538\n",
      "Epoch 17/100\n",
      "1929/1929 [==============================] - 2s 1ms/step - loss: 1.2769 - accuracy: 0.5769 - val_loss: 1.3682 - val_accuracy: 0.5537\n",
      "414/414 [==============================] - 0s 768us/step - loss: 1.3593 - accuracy: 0.5496\n"
     ]
    }
   ],
   "source": [
    "## second neural network.\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir = root / 'src' / 'logs',histogram_freq=1) \n",
    "\n",
    "base_model_deep = Sequential(\n",
    "    [   \n",
    "        Dense(23,activation='relu', name = 'input',input_shape=(len(x_train[0]),)),\n",
    "        Dense(150,activation='relu'),\n",
    "        Dense(50,activation='relu'),\n",
    "        Dense(50,activation='relu'),\n",
    "        Dense(50,activation='relu'),\n",
    "        Dense(15, activation='softmax',name='output')\n",
    "    ]\n",
    ")\n",
    "\n",
    "base_model_deep.build()\n",
    "base_model_deep.summary()\n",
    "\n",
    "base_model_deep.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "base_model_deep.fit(x_train,y_train,epochs=100,validation_data=(x_valid,y_valid),callbacks=[early_stop,tensorboard_callback],verbose=1)\n",
    "\n",
    "score['base_deep'] = base_model_deep.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunetaly adding more layers even slightly decreases the performance. During the hypertuning step we will see if we can find an optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\huube\\OneDrive\\Master of Informatics\\Machine Learning\\Eindopdracht\\src\\models\\best_base_model.model\\assets\n"
     ]
    }
   ],
   "source": [
    "## Saving base model\n",
    "file_model = root / 'src' / 'models' / 'best_base_model.model' \n",
    "base_model.save(file_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dummy_baseline': 0.3842256503327284,\n",
       " 'base': [1.3469150066375732, 0.5529340505599976],\n",
       " 'base2': [1.3545866012573242, 0.5489261746406555]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Conclusions </h4>\n",
    "\n",
    "<p> So in this notebook we tried a few models and saved the best one for the hypertuning phase. The simple base model seemed to be performing slightly better but by such a small margin that we let the hypertuner figure this out for us."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb5c4cecea5d0cc7b6f195879a08e1325a36c50d87ab099289b540df622448c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
