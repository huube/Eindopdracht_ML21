{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hypertuning using Keras-tuner</h2>\n",
    "\n",
    "In this notebook, we will try to find the optimal model parameters by using keras-tuner. We tried Ray but we couldn't get that one to work. Not locally but also not in Google Colab.\n",
    "We will try two hypertunes: randomsearch and hyperband. The best hypertuner will deliver the \"winning\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "from loguru import logger\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Input, Dropout\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.tuners import RandomSearch, Hyperband\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters as hp\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from definitions import get_project_root\n",
    "from src.data.make_dataset import create_train_test_validation\n",
    "from src.visualization.visualize import plot_results\n",
    "\n",
    "root = get_project_root()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 13:04:28.445 | INFO     | src.data.make_dataset:create_train_test_validation:73 - found file labeled_data.csv, procceed with creating train, test and validation sets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((61711, 23), (61711, 1), (13225, 23), (13225, 1), (13224, 23), (13224, 1))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create train, validation and test sets\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = create_train_test_validation()\n",
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(23,activation='relu', input_shape=(23,)))\n",
    "\n",
    "    for i in range(hp.Int(\"n_layers\",1,6)):\n",
    "        model.add(Dense(hp.Int(f\"layer_{i}\",5,500,step=50),activation='relu'))\n",
    "\n",
    "    model.add(Dropout(hp.Float(\"dropout\",0.05,0.4)))\n",
    "    model.add(Dense(15,activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> randomsearch tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 02m 27s]\n",
      "val_loss: 1.3695966402689617\n",
      "\n",
      "Best val_loss So Far: 1.3497042258580525\n",
      "Total elapsed time: 00h 34m 56s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "## Tune the model by using RandomSearch\n",
    "log_dir = \"logs_kt_random\"\n",
    "tensorboard = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    model_builder,\n",
    "    objective='val_loss',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=3,\n",
    "    directory=log_dir)\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(x=x_train,y=y_train,epochs=10,batch_size=64,callbacks=[tensorboard],validation_data=(x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the best hyperparameters as a variable\n",
    "best_randomsearch_params = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_builder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\huube\\OneDrive\\Master of Informatics\\Machine Learning\\Eindopdracht\\notebooks\\3-hypertune.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/notebooks/3-hypertune.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39m## Train a model with the best hyperparemeters and save the score of the randomsearch model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/notebooks/3-hypertune.ipynb#ch0000007?line=2'>3</a>\u001b[0m randomsearch_model \u001b[39m=\u001b[39m model_builder(best_randomsearch_params)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/notebooks/3-hypertune.ipynb#ch0000007?line=3'>4</a>\u001b[0m history_random \u001b[39m=\u001b[39m randomsearch_model\u001b[39m.\u001b[39mfit(x_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,validation_data\u001b[39m=\u001b[39m(x_valid,y_valid),callbacks\u001b[39m=\u001b[39m[EarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_builder' is not defined"
     ]
    }
   ],
   "source": [
    "## Train a model with the best hyperparemeters and save the score of the randomsearch model\n",
    "\n",
    "randomsearch_model = model_builder(best_randomsearch_params)\n",
    "history_random = randomsearch_model.fit(x_train, y_train, epochs=50,validation_data=(x_valid,y_valid),callbacks=[EarlyStopping(patience=5,restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414/414 [==============================] - 1s 1ms/step - loss: 1.3500 - accuracy: 0.5537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3500087261199951, 0.5536902546882629]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomsearch_result = randomsearch_model.evaluate(x_test,y_test)\n",
    "randomsearch_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the score of the randomsearch model is lower compared to the base model. Because the randomsearch is in fact searching quite random and we do not have very good resources for hypertuning, we will move on with the hyperband which can be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> We can reuse the model_builder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 01m 08s]\n",
      "val_loss: 1.3541325330734253\n",
      "\n",
      "Best val_loss So Far: 1.3442531824111938\n",
      "Total elapsed time: 00h 16m 39s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "## Hypertune by using the hyperband tuner\n",
    "\n",
    "log_dir = \"kt_hb_acc2\"\n",
    "\n",
    "tuner_hb = Hyperband(\n",
    "    model_builder,\n",
    "    objective=\"val_loss\",\n",
    "    max_epochs=25,\n",
    "    factor=3,\n",
    "    hyperband_iterations=1,\n",
    "    seed=42,\n",
    "    directory=log_dir\n",
    ")\n",
    "\n",
    "tuner_hb.search(x=x_train,y=y_train,epochs=12,batch_size=64,callbacks=[tensorboard],validation_data=(x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'maximize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\huube\\OneDrive\\Master of Informatics\\Machine Learning\\Eindopdracht\\notebooks\\3-hypertune.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/notebooks/3-hypertune.ipynb#ch0000023?line=0'>1</a>\u001b[0m best_hyperband_params \u001b[39m=\u001b[39m tuner_hb\u001b[39m.\u001b[39mget_best_hyperparameters(num_trials\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/notebooks/3-hypertune.ipynb#ch0000023?line=1'>2</a>\u001b[0m hyperband_model \u001b[39m=\u001b[39m model_builder(best_randomsearch_params)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/notebooks/3-hypertune.ipynb#ch0000023?line=2'>3</a>\u001b[0m history_hb \u001b[39m=\u001b[39m hyperband_model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m(x_valid,y_valid),callbacks\u001b[39m=\u001b[39;49m[EarlyStopping(patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,restore_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)],maximize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\huube\\OneDrive\\Master of Informatics\\Machine Learning\\Eindopdracht\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\huube\\OneDrive\\Master of Informatics\\Machine Learning\\Eindopdracht\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/huube/OneDrive/Master%20of%20Informatics/Machine%20Learning/Eindopdracht/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'maximize'"
     ]
    }
   ],
   "source": [
    "## Test the parameters by training the model with these.\n",
    "best_hyperband_params = tuner_hb.get_best_hyperparameters(num_trials=1)[0]\n",
    "hyperband_model = model_builder(best_randomsearch_params)\n",
    "history_hb = hyperband_model.fit(x_train, y_train, epochs=50,validation_data=(x_valid,y_valid),callbacks=[EarlyStopping(patience=5,restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414/414 [==============================] - 1s 1ms/step - loss: 1.3596 - accuracy: 0.5526\n"
     ]
    }
   ],
   "source": [
    "## Test the model on the test data\n",
    "\n",
    "hyperband_result = hyperband_model.evaluate(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_6_layer_call_and_return_conditional_losses, dense_6_layer_call_fn, dense_7_layer_call_and_return_conditional_losses, dense_7_layer_call_fn, dense_8_layer_call_and_return_conditional_losses while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\huube\\OneDrive\\Master of Informatics\\Machine Learning\\Eindopdracht\\src\\models\\winning_hypermodel.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\huube\\OneDrive\\Master of Informatics\\Machine Learning\\Eindopdracht\\src\\models\\winning_hypermodel.model\\assets\n"
     ]
    }
   ],
   "source": [
    "file_model = root / 'src' / 'models' / 'winning_hypermodel.model'\n",
    "randomsearch_model.save(file_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Conclusion </h2>\n",
    "Unfortunately we ran out of time because I think there is more to gain from the hypertune step. I found this to be the most complex stuff so far. I think there is some optimzation to do in the search space by providing better parameters. It seems quite random that my initial simple model outperforms a hypertuner. But as mentioned, because of time I must move on with the evaluation."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb5c4cecea5d0cc7b6f195879a08e1325a36c50d87ab099289b540df622448c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
