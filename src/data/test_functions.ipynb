{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import layers\n",
    "from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder, StandardScaler\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "root = Path()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_data():\n",
    "    \"\"\"This function creates the labeled dataset by combining the two files based on the spotify ID\"\"\"\n",
    "    \n",
    "    \n",
    "    ## Define filepaths to directories and files\n",
    "    data_dir = root / 'raw'\n",
    "    result_dir = root / 'processed'\n",
    "\n",
    "    meta_file = data_dir / 'spotify_id_metadata.csv'\n",
    "    trackid_file = data_dir / 'tracks_with_spotifyid.csv'\n",
    "    label_file = data_dir / 'songlist.csv'\n",
    "\n",
    "    ## Where resulting dataset will be written to as csv\n",
    "    result_file = result_dir / 'labeled_data.csv'\n",
    "\n",
    "    ## read in source files\n",
    "    df_meta = pd.read_csv(meta_file)\n",
    "    df_trackid = pd.read_csv(trackid_file)\n",
    "    df_label = pd.read_csv(label_file,usecols = ['track_id','duration','genre'])\n",
    "\n",
    "    ## Combine file with metadata on tracks with the original songlist ID\n",
    "    ## and drop some irrelevant columns\n",
    "    df = pd.merge(df_meta,df_trackid,left_on='id',right_on='spotify_id',how='inner')\n",
    "\n",
    "    columns_to_drop = ['track_href','search_artist','search_track','id','Unnamed: 0']\n",
    "    df.drop(columns=columns_to_drop,inplace=True)\n",
    "    \n",
    "    ## Merge the music genre labels with the tracks including metadata. \n",
    "    df_out = pd.merge(df,df_label,left_on='track_id',right_on='track_id',how='inner')\n",
    "\n",
    "    df_out.to_csv(result_file,index=False)\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_validation():\n",
    "    \"\"\"This function will create a train, test and validation set \"\"\"\n",
    "\n",
    "    data_dir : Path = root / 'processed'\n",
    "    input_file : str = 'labeled_data.csv'\n",
    "    file_path : Path = data_dir / input_file\n",
    "\n",
    "    if file_path.exists():\n",
    "        logger.info(f\"found file {input_file}, procceed with creating train, test and validation sets\")\n",
    "        try: \n",
    "            df = pd.read_csv(file_path)\n",
    "        except:\n",
    "            logger.info(f\"an error occured while trying to pd.read_csv {file_path}\")\n",
    "\n",
    "    else:\n",
    "        logger.info('Labeled data not found, creating a new file.')\n",
    "        create_labeled_data()\n",
    "    \n",
    "    ## remove these columns\n",
    "    columns_to_drop = [\n",
    "        'type',\n",
    "        'uri',\n",
    "        'analysis_url',\n",
    "        'time_signature',\n",
    "        'track_id',\n",
    "        'response_artist',\n",
    "        'response_track',\n",
    "        'spotify_id',\n",
    "    ]\n",
    "\n",
    "    df.drop(columns=columns_to_drop)\n",
    "\n",
    "    ## make these columns categorical variables\n",
    "\n",
    "    for col in ['key','mode','genre']:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "\n",
    "    ##initialize encoders\n",
    "\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    scaler = StandardScaler()\n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    ## Transform genre to ordinal coding\n",
    "    ordinaL_columns = oe.fit_transform(df['genre'])\n",
    "\n",
    "    ## apply OneHot encoding to categorical variables\n",
    "    columns_to_onehot = ['key']\n",
    "    onehot_columns = ohe.fit_transform(df[columns_to_onehot])\n",
    "\n",
    "    ## Apply StandardScaler to numeric values\n",
    "    columns_to_scale = [\n",
    "        'danceabilitiy',\n",
    "        'energy',\n",
    "        'loudness',\n",
    "        'speechiness',\n",
    "        'acousticness',\n",
    "        'instrumentalness',\n",
    "        'liveness',\n",
    "        'valence',\n",
    "        'tempo',\n",
    "        'duration_ms'\n",
    "    ]\n",
    "    \n",
    "    scaled_columns = scaler.fit_transform(columns_to_scale)\n",
    "\n",
    "    ## Not forget about the already binary feature 'mode'\n",
    "\n",
    "    binary_columns = df['mode'].to_numpy()\n",
    "\n",
    "    dataset = np.concatenate([onehot_columns,scaled_columns,binary_columns,ordinaL_columns],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 22:11:21.157 | INFO     | __main__:<module>:6 - found file labeled_data.csv, procceed with creating train, test and validation sets\n"
     ]
    }
   ],
   "source": [
    "data_dir : Path = root / 'processed'\n",
    "input_file : str = 'labeled_data.csv'\n",
    "file_path : Path = data_dir / input_file\n",
    "\n",
    "if file_path.exists():\n",
    "    logger.info(f\"found file {input_file}, procceed with creating train, test and validation sets\")\n",
    "    try: \n",
    "        df = pd.read_csv(file_path)\n",
    "    except:\n",
    "        logger.info(f\"an error occured while trying to pd.read_csv {file_path}\")\n",
    "\n",
    "else:\n",
    "    logger.info('Labeled data not found, creating a new file.')\n",
    "    create_labeled_data()\n",
    "\n",
    "## remove these columns\n",
    "columns_to_drop = [\n",
    "    'type',\n",
    "    'uri',\n",
    "    'analysis_url',\n",
    "    'time_signature',\n",
    "    'track_id',\n",
    "    'response_artist',\n",
    "    'response_track',\n",
    "    'spotify_id',\n",
    "]\n",
    "\n",
    "df.drop(columns=columns_to_drop,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make these columns categorical variables\n",
    "\n",
    "for col in ['key','mode','genre']:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "\n",
    "##initialize encoders\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "oe = OrdinalEncoder()\n",
    "\n",
    "## Transform genre to ordinal coding\n",
    "ordinal_columns = oe.fit_transform(df['genre'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88160, 24)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## apply OneHot encoding to categorical variables\n",
    "columns_to_onehot = ['key']\n",
    "onehot_columns = ohe.fit_transform(df[columns_to_onehot])\n",
    "\n",
    "## Apply StandardScaler to numeric values\n",
    "columns_to_scale = [\n",
    "    'danceability',\n",
    "    'energy',\n",
    "    'loudness',\n",
    "    'speechiness',\n",
    "    'acousticness',\n",
    "    'instrumentalness',\n",
    "    'liveness',\n",
    "    'valence',\n",
    "    'tempo',\n",
    "    'duration_ms'\n",
    "]\n",
    "\n",
    "scaled_columns = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "## Not forget about the already binary feature 'mode'\n",
    "\n",
    "binary_columns = df['mode'].to_numpy().reshape(-1,1)\n",
    "\n",
    "dataset = np.concatenate([onehot_columns,scaled_columns,binary_columns,ordinal_columns],axis=1)\n",
    "\n",
    "dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[:,-1]\n",
    "x = dataset[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((88160, 23), (88160, 1))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "x.shape,y.shape\n",
    "\n",
    "x_train, y_train = x[]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb5c4cecea5d0cc7b6f195879a08e1325a36c50d87ab099289b540df622448c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
